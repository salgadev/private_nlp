{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted Training Demo on Medical Text Data using SyferText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:**\n",
    "- Carlos Salgado - [email](mailto:csalgado@uwo.ca) | [GitHub](https://github.com/socd06) | [LinkedIn](www.linkedin.com/in/eng-socd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Bob <sup>MD</sup> and Alice <sup>MD</sup> are physicians running their respective medical practices and both have a database of private medical transcriptions. You own a Natural Language Processing (NLP) company and have been contacted by these physicians because both Bob <sup>MD</sup> and Alice <sup>MD</sup> have heard of the high quality of the Machine Learning as a Service (MLaaS) solutions you provide and want you to create a text classifier to help them automatically assign a medical specialty to each new patient text transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "Healthcare data is highly regulated and should be, for most intents and purposes, private. Therefore, if in a medical setting, the Machine Learning model being trained should not actually look at the data. \n",
    "\n",
    "Combining both Bob's and Alice's datasets, you should be able to create a bigger, better dataset that you could use to train your model with higher accuracy, only that you can't because it's all sensitive and private data, which is why you will need [PySyft](https://github.com/OpenMined/pysyft/) and [SyferText](https://github.com/OpenMined/SyferText/) to complete the job at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to first install [PySyft](https://github.com/OpenMined/PySyft) and [SyferText](https://github.com/OpenMined/SyferText) before you run this tutorial. \n",
    "Using virtual environments is highly recommended for any PySyft experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying requirements and installing if missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <p style='color:red;'> (IGNORE THIS STEP IF YOU CLONED THE REPO OR ALREADY DOWNLOADED THE DATASET) </p>\n",
    "</div>\n",
    "\n",
    "The dataset will be downloaded in a folder called `data` in the root directory. Three files will be downloaded using the `download_dataset` helper function:\n",
    "\n",
    "- `mtsamples.csv`: This is the dataset file containing almost 5K sample medical transcriptions. It is a csv file composed of five columns: `description`,`medical_specialty`,`sample_name`,`transcription`, and `keywords`. The `transcription` column holds the sample text, and the `medical_specialty` contains the labels we will use to train the classifier. \n",
    "\n",
    "- `clinical-stopwords.txt`: Clinical stop words compiled by [Dr. Kavita Ganesan](https://github.com/kavgan) from the [clinical-concepts](https://github.com/kavgan/clinical-concepts) repository. \n",
    "\n",
    "- `vocab.txt`: Vocabulary text file generated using the Systematized Nomenclature of Medicine International (SNMI) data.\n",
    "\n",
    "- `mt.csv`: Reduced dataset containing only the `medical_specialty` and `transcription` features.\n",
    "\n",
    "Please run the cell below in order to download the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataset downloader helper function from the `scripts` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from util import download_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL template to all dataset files\n",
    "url_template = 'https://raw.githubusercontent.com/socd06/medical-nlp/master/data/%s'\n",
    "\n",
    "# File names to be downloaded from the using the URL template above\n",
    "files = ['mtsamples.csv', 'clinical-stopwords.txt', 'vocab.txt', 'mt.csv']\n",
    "\n",
    "# Construct the list of urls\n",
    "urls = [url_template % file for file in files]\n",
    "\n",
    "# The dataset name and its root folder\n",
    "dataset_name = 'data'\n",
    "root_path = '../data'\n",
    "\n",
    "# Create the dataset folder if it is not already there\n",
    "if not os.path.exists('../data'):\n",
    "    os.mkdir('../data')\n",
    "\n",
    "# Start downloading\n",
    "download_dataset(dataset_name = dataset_name, \n",
    "                 urls = urls, \n",
    "                 root_path = root_path\n",
    "                )\n",
    "print(\"Succesfully downloaded:\",files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.15.3.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySyft imports\n",
    "import syft as sy\n",
    "from syft.generic.string import String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# SyferText imports\n",
    "import syfertext\n",
    "from syfertext.pipeline import SimpleTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from pprint import pprint\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the work environment\n",
    "In this part, we assume each client owns a part of the full dataset and we prepare each worker to perform encrypted training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual environment\n",
    "\n",
    "If you have not setup [PyGrid](https://github.com/OpenMined/PyGrid/) yet or want to only focus on the training aspect of this demo, run the cell below and skip the next cell.\n",
    "\n",
    "A work environment is simulated with three main actors, a company (us) and two clients owning two private datasets (Bob and Alice) but also a crypto provider which will provide the primitives for Secure Multi-Party Computation (SMPC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "# Create a torch hook for PySyft\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Create some PySyft workers\n",
    "me = hook.local_worker # This is the worker representing the deep learning company\n",
    "bob = sy.VirtualWorker(hook, id = 'bob') # Bob owns the first dataset\n",
    "alice = sy.VirtualWorker(hook, id = 'alice') # Alice owns the second dataset\n",
    "\n",
    "crypto_provider = sy.VirtualWorker(hook, id = 'crypto_provider') # provides encryption primitive for SMPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local PyGrid environment\n",
    "\n",
    "A local grid work environment is initialized using [PyGrid](https://github.com/OpenMined/PyGrid/) with three main actors, a company (us) and two clients owning two private datasets (Bob and Alice) but also a crypto provider which will provide the primitives for Secure Multi-Party Computation (SMPC). \n",
    "**Warning: Highly Experimental**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.workers.node_client import NodeClient\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "me = NodeClient(hook, \"ws://localhost:3000\")\n",
    "bob = NodeClient(hook, \"ws://localhost:3001\")\n",
    "alice = NodeClient(hook, \"ws://localhost:3002\")\n",
    "\n",
    "crypto_provider = NodeClient(hook, \"ws://localhost:3003\")\n",
    "\n",
    "my_grid = sy.PrivateGridNetwork(me, bob, alice, crypto_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset file\n",
    "dataset_path = '../data/mt.csv'\n",
    "\n",
    "# store the dataset as a list of dictionaries\n",
    "# each dictionary has two keys, 'transcription' and 'label'\n",
    "# the 'transcription' element is a PySyft String\n",
    "# the 'label' element is an integer with 1 for each surgical specialty and a 0 otherwise\n",
    "dataset_local = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_path, 'r') as dataset_file:\n",
    "    \n",
    "    # Create a csv reader object\n",
    "    reader = csv.DictReader(dataset_file)\n",
    "    \n",
    "    for elem in reader:\n",
    "        \n",
    "        # Create one entry\n",
    "        # Check if the medical specialty contains \"urgery\" \n",
    "        # meaning the transcription could be labeled \"Surgery\",\"Cosmetic / Plastic Surgery\" or \"Neurosurgery\"\n",
    "        example = dict(transcription = String(elem['transcription']),                       \n",
    "                       label = 1 if 'urgery' in elem['medical_specialty'] else 0 \n",
    "                      )\n",
    "        \n",
    "        # add to the local dataset\n",
    "        dataset_local.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how an element in the list looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed to generate random integer numbers\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry # 1893\n",
      "{'label': 0,\n",
      " 'transcription': \"ADMITTING DIAGNOSES:,1.  Fever.,2.  Otitis media.,3.  Possible sepsis.,HISTORY OF PRESENT ILLNESS:  ,The patient is a 10-month-old male who was seen in the office 1 day prior to admission.  He has had a 2-day history of fever that has gone up to as high as 103.6 degrees F. He has also had intermittent cough, nasal congestion, and rhinorrhea and no history of rashes.  He has been taking Tylenol and Advil to help decrease the fevers, but the fever has continued to rise.  He was noted to have some increased workup of breathing and parents returned to the office on the day of admission.,PAST MEDICAL HISTORY: , Significant for being born at 33 weeks' gestation with a birth weight of 5 pounds and 1 ounce.,PHYSICAL EXAMINATION: , On exam, he was moderately ill appearing and lethargic.  HEENT:  Atraumatic, normocephalic.  Pupils are equal, round, and reactive to light.  Tympanic membranes were red and yellow, and opaque bilaterally.  Nares were patent.  Oropharynx was slightly moist and pink.  Neck was soft and supple without masses.  Heart is regular rate and rhythm without murmurs.  Lungs showed increased workup of breathing, moderate tachypnea.  No rales, rhonchi or wheezes were noted.  Abdomen:  Soft, nontender, nondistended.  Active bowel sounds.  Neurologic exam showed good muscle strength, normal tone.  Cranial nerves II through XII are grossly intact.,LABORATORY FINDINGS: , He had electrolytes, BUN and creatinine, and glucose all of which were within normal limits.  White blood cell count was 8.6 with 61% neutrophils, 21% lymphocytes, 17% monocytes, suggestive of a viral infection.  Urinalysis was completely unremarkable.  Chest x-ray showed a suboptimal inspiration, but no evidence of an acute process in the chest.,HOSPITAL COURSE: , The patient was admitted to the hospital and allowed a clear liquid diet.  Activity is as tolerates.  CBC with differential, blood culture, electrolytes, BUN, and creatinine, glucose, UA, and urine culture all were ordered.  Chest x-ray was ordered as well with 2 views to evaluate for a possible pneumonia.  Pulse oximetry checks were ordered every shift and as needed with O2 ordered per nasal cannula if O2 saturations were less that 94%.  Gave D5 and quarter of normal saline at 45 mL per hour, which was just slightly above maintenance rate to help with hydration.  He was given ceftriaxone 500 mg IV once daily to treat otitis media and possible sepsis, and I will add Tylenol and ibuprofen as needed for fevers.  Overnight, he did have his oxygen saturations drop and went into oxygen overnight.  His lungs remained clear, but because of the need for O2, we instituted albuterol aerosols every 6 hours to help maintain good lung function.  The nurses were instructed to attempt to wean O2 if possible and advance the diet.  He was doing clear liquids well and so I saline locked to help to accommodate improve the mobility with the patient.  He did well the following evening with no further oxygen requirement.  He continued to spike fevers but last fever was around 13:45 on the previous day.  At the time of exam, he had 100% oxygen saturations on room air with temperature of 99.3 degrees F. with clear lungs.  He was given additional dose of Rocephin when it was felt that it would be appropriate for him to be discharged that morning.,CONDITION OF THE PATIENT AT DISCHARGE: , He was at 100% oxygen saturations on room air with no further dips at night.  He has become afebrile and was having no further increased work of breathing.,DISCHARGE DIAGNOSES:,1.  Bilateral otitis media.,2.  Fever.,PLAN:  ,Recommended discharge.  No restrictions in diet or activity.  He was continued Omnicef 125 mg/5 mL one teaspoon p.o. once daily and instructed to follow up with Dr. X, his primary doctor, on the following Tuesday.  Parents were instructed also to call if new symptoms occurred or he had return if difficulties with breathing or increased lethargy.\"}\n"
     ]
    }
   ],
   "source": [
    "# Get a random index to verify entry examples\n",
    "random_index = randint(0,len(dataset_local))\n",
    "print(\"Entry #\",random_index)\n",
    "example = dataset_local[random_index]\n",
    "pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we verified our examples we can look into our data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'syft.generic.string.String'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(example['transcription']))\n",
    "print(type(example['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transcription text is a PySyft `String` object. The label is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributing documents privately\n",
    "\n",
    "We simulate two private datasets owned by two clients (Bob and Alice):\n",
    "\n",
    "1. Load the whole dataset in `mtsamples.csv` locally (the `me` worker). The data will be loaded as a list of dictionaries that has the following format: `[ {'transcription': <transcription text>, 'label': <0 or 1>}, {...}, {...}]`\n",
    "\n",
    "\n",
    "2. Split the dataset into two parts, one for Bob and the other for Alice. Each part will be also split into a training set and a validation set. This will create four lists: `train_bob`, `valid_bob`, `train_alice`, `valid_alice`. Each list has the same format mentioned above.\n",
    "\n",
    "\n",
    "3. Each element in the four lists will be sent to the corresponding worker. This will change the content of the lists as depicted in **Figure 1**. Each list willl hold PySyft pointers to the texts and labels instead of the objects themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<img alt = 'medical transcriptions remote datasets' src ='./img/mtsamples_remote.png' style='width:700px'>\n",
    "<div>\n",
    "<div style='width:600px;margin:30px auto 10px auto;text-align:center;'>\n",
    "<strong> Figure 1: </strong> The transcription text and their labels are remotely located in Bob and Alice's remote worker machines, only pointers to them are kept by the local worker (the company's machine).\n",
    "</div>\n",
    "</div>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into two equal parts and send each part to a different worker simulating two remote datasets mentioned before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two datasets, one for Bob and another one for Alice\n",
    "dataset_bob, dataset_alice = train_test_split(dataset_local, train_size = 0.5)\n",
    "\n",
    "# Now create a validation set for Bob and another one for Alice\n",
    "train_bob, val_bob = train_test_split(dataset_bob, train_size = 0.7)\n",
    "train_alice, val_alice = train_test_split(dataset_alice, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the datasets remote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that sends the content of each split to a remote worker\n",
    "def make_remote_dataset(dataset, worker):\n",
    "\n",
    "    # Got through each example in the dataset\n",
    "    for example in dataset:\n",
    "        \n",
    "        # Send each transcription text\n",
    "        example['transcription'] = example['transcription'].send(worker)\n",
    "                       \n",
    "        # Send each label as a one-hot-encoded vector\n",
    "        one_hot_label = torch.zeros(2).scatter(0, torch.Tensor([example['label']]).long(), 1)\n",
    "        \n",
    "        # print for debugging purposes\n",
    "        # print(\"mapping\",example['label'],\" to \",one_hot_label)\n",
    "        \n",
    "        # Send the transcription label\n",
    "        example['label'] = one_hot_label.send(worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function transforms the label to a one-hot-encoded format before sending it to a remote worker. Every label corresponds to a 2-digit tensor of binary values (`[1,0]` or `[0,1]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert the dataset into a remote dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bob's remote dataset\n",
    "make_remote_dataset(train_bob, bob)\n",
    "make_remote_dataset(val_bob, bob)\n",
    "\n",
    "# Alice's remote dataset\n",
    "make_remote_dataset(train_alice, alice)\n",
    "make_remote_dataset(val_alice, alice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Bob's dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'syft.generic.pointers.string_pointer.StringPointer'>\n",
      "(Wrapper)>[PointerTensor | me:38511952257 -> bob:59285874892]\n"
     ]
    }
   ],
   "source": [
    "# Take an element from the dataset\n",
    "example = train_bob[0]\n",
    "\n",
    "print(type(example['transcription']))\n",
    "print(example['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text type is a PySyft `StringPointer` that points to the real `String` object  located in Bob's machine. The label type is a PySyft `PointerTensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can review and see where the label is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VirtualWorker id:bob #objects:4972>\n",
      "<VirtualWorker id:bob #objects:4972>\n"
     ]
    }
   ],
   "source": [
    "print(example['transcription'].location)\n",
    "print(example['label'].location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirming that the dataset is now remote and also confirming the information in **Figure 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the environment and the data are ready for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a `SyferText` Language object and a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Language object with SyferText\n",
    "nlp = syfertext.load('en_core_web_lg', owner = me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you create a Language object a pipeline will be created. At initialization, a pipeline only contains a tokenizer. You can see this for yourself using the `pipeline_template` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'remote': True, 'name': 'tokenizer'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syfertext.language.Language"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tokenizer entry has a propery called `remote` set to `True`. This means we allowed the tokenizer to be sent to a remote worker for the string to be tokenized there.\n",
    "\n",
    "We can add more components to the pipeline by using the `add_pipe` method of the Language class. One component we can add is a `SimpleTagger` object. This is a SyferText object that we can use to set custom attributes to individual tokens. In this tutorial, we will create two taggers: One that tags tokens that are Stop Words and another one that tags each token with their respective class.\n",
    "\n",
    "By tagging we mean setting a custom attribute to a token and assigning it a given value (e.g. An attribute called `is_stop` with `True` and `False` values when evaluating Stop Words. \n",
    "\n",
    "You can refer to **Figure 2** to see how a pipeline is distributed on multiple workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the taggers to the pipeline\n",
    "The `excluded_tokens` dictionary will be used further down, when we create embedding vectors for the transcriptions. This dictionary will enable us to exclude some tokens when we create a document embedding. Such exclusion will be based on the value of the custom attributes we set with the taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_stop_tagger = True\n",
    "use_vocab_tagger = True\n",
    "\n",
    "# Token with these custom tags\n",
    "# will be excluded from creating\n",
    "# the Doc vector\n",
    "excluded_tokens = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a  tagger for stop words\n",
    "We will start by creating the Stop Word tagger. First loading the stop word file into a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of stop words\n",
    "with open('../data/clinical-stopwords.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the tagger which is an object of the `SimpleTagger` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple tagger object to tag stop words\n",
    "stop_tagger = SimpleTagger(attribute = 'is_stop',\n",
    "                           lookups = stop_words,\n",
    "                           tag = True,\n",
    "                           default_tag = False,\n",
    "                           case_sensitive = False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `lookups` argument passed was the list of stop words.\n",
    "\n",
    "Every token in the `Doc` object will be given a custom attribute called `is_stop`. Every time a stop word is found, this attribute will be given the value `True` specified by the `tag` argument of the `SimpleTagger` class initializer, otherwise, the `default_tag` will be used (e.g. `False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_stop_tagger:\n",
    "\n",
    "    # Add the stop word to the pipeline\n",
    "    nlp.add_pipe(name = 'stop tagger',\n",
    "                 component = stop_tagger,\n",
    "                 remote = True\n",
    "                )\n",
    "\n",
    "    # Tokens with 'is_stop' = True are\n",
    "    # not going to be used when creating the \n",
    "    # Doc vector\n",
    "    excluded_tokens['is_stop'] = {True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a  tagger for vocab words\n",
    "Likewise we create a SimpleTagger instance that filters out tokens not in our vocabulary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/vocab.txt', 'r') as f:\n",
    "    vocab_words = f.read().splitlines()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple tagger object to tag stop words\n",
    "vocab_tagger = SimpleTagger(attribute = 'is_vocab',\n",
    "                           lookups = vocab_words,\n",
    "                           tag = True,\n",
    "                           default_tag = False,\n",
    "                           case_sensitive = False\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_vocab_tagger:\n",
    "\n",
    "    # Add the stop word to the pipeline\n",
    "    nlp.add_pipe(name = 'vocab tagger',\n",
    "                 component = vocab_tagger,\n",
    "                 remote = True\n",
    "                )\n",
    "\n",
    "    # Tokens with 'is_vocab' = False are\n",
    "    # not going to be used when creating the \n",
    "    # Doc vector\n",
    "    excluded_tokens['is_vocab'] = {False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what pipe components are included in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'remote': True, 'name': 'tokenizer'},\n",
       " {'remote': True, 'name': 'stop tagger'},\n",
       " {'remote': True, 'name': 'vocab tagger'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset class\n",
    "\n",
    "Now that the datasets are remote and ready along with the `Language` object and its pipeline we can create PyTorch loaders to make data batches for training and validation.\n",
    "\n",
    "The batches will be composed of training examples coming from both Bob's and Alice's datasets as if it were only one big dataset.\n",
    "\n",
    "Each example in the batch contains an encrypted version of one transcription's embedding vector and its encrypted label. For this tutorial, the vector will be computed as the average of the transcription's individual token vectors taken from the `en_core_web_lg` language model. Also, tokens with custom tags indicated in `excluded_tokens` won't be taken into account in computing a transcription's vector.\n",
    "\n",
    "From **Figure 2** we can see how the transcription text is remotely preprocessed by SyferText: \n",
    "\n",
    "1. First, the `Language` object `nlp` is used to preprocess one transcription on Bob's or Alice's machine.\n",
    "2. The object `nlp` determines that the real transcription text is actually remote, so it sends a subpipeline containing the required pipeline components we defined to the corresponding worker.\n",
    "3. The subpipeline is run and a `Doc` object is created on the remote worker containing the transcription's individual tokens appropriately tokenized and tagged.\n",
    "4. On the local worker, a `DocPointer` object is created pointing to that `Doc` object.\n",
    "5. By calling `get_encrypted_vector()` on the `DocPointer`, the call is forwarded to `Doc`, which, in turn, computes the `Doc` vector, encrypts it with Secure Multy-Party Computation (SMPC) using PySyft and returns it to the caller at the local worker.\n",
    "6. The PyTorch dataloader takes this encrypted vector and appends it to the training or validation batch.\n",
    "\n",
    "Note that at no moment in the process, the plaintext data of the remote datasets are revealed to the local worker. *Privacy is preserved thanks to SyferText and PySyft!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<br>\n",
    "<img alt =  'SyferText pipeline' src ='./img/mt_pipeline.png' style='width:700px;'>\n",
    "<div>\n",
    "<p style='width:600px;margin:30px auto 10px auto;text-align:center;'>\n",
    "<strong> Figure 2: </strong> A pipeline on the local worker only contains pointers to subpipelines carrying out the actual preprocessing on remote workers.\n",
    "</p>\n",
    "</div>\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a minute to review the `__getitem__()` method of the custom PyTorch `Dataset` object defined below and see how. Please take a few minutes to check it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMTS(Dataset):\n",
    "    \n",
    "    def __init__(self, sets, share_workers, crypto_provider, nlp):\n",
    "        \"\"\"Initialize the Dataset object\n",
    "        \n",
    "        Args:\n",
    "            sets (list): A list containing all training OR \n",
    "                all validation sets to be used.\n",
    "            share_workers (list): A list of workers that will\n",
    "                be used to hold the SMPC shares.\n",
    "            crypto_provider (worker): A worker that will \n",
    "                provide SMPC primitives for encryption.\n",
    "            nlp: This is SyferText's Language object containing\n",
    "                the preprocessing pipeline.\n",
    "        \"\"\"\n",
    "        self.sets = sets\n",
    "        self.crypto_provider = crypto_provider\n",
    "        self.workers = share_workers\n",
    "    \n",
    "        # Create a single dataset unifying all datasets.\n",
    "        # A property called `self.dataset` is created \n",
    "        # as a result of this call.\n",
    "        self._create_dataset()\n",
    "        \n",
    "        # The language model\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"In this function, preprocessing with SyferText \n",
    "        of one transcription will be triggered. Encryption will also\n",
    "        be performed and the encrypted vector will be obtained.\n",
    "        The encrypted label will be computed too.\n",
    "        \n",
    "        Args:\n",
    "            index (int): This is an integer received by the \n",
    "                PyTorch DataLoader. It specifies the index of\n",
    "                the example to be fetched. This actually indexes\n",
    "                one example in `self.dataset` which pools over\n",
    "                examples of all the remote datasets.\n",
    "        \"\"\"\n",
    "        \n",
    "        # get the example\n",
    "        example = self.dataset[index]\n",
    "        \n",
    "        # Run the preprocessing pipeline on \n",
    "        # the transcription text and get a DocPointer object\n",
    "        doc_ptr = self.nlp(example['transcription'])\n",
    "        \n",
    "        # Get the encrypted vector embedding for the document\n",
    "        vector_enc = doc_ptr.get_encrypted_vector(bob, \n",
    "                                                  alice, \n",
    "                                                  crypto_provider = self.crypto_provider,\n",
    "                                                  requires_grad = True,\n",
    "                                                  excluded_tokens = excluded_tokens\n",
    "                                                 )\n",
    "        \n",
    "\n",
    "        # Encrypt the target label\n",
    "        label_enc = example['label'].fix_precision().share(bob, \n",
    "                                                           alice, \n",
    "                                                           crypto_provider = self.crypto_provider,\n",
    "                                                           requires_grad = True\n",
    "                                                          ).get()\n",
    "\n",
    "\n",
    "        return vector_enc, label_enc\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the combined size of all of the \n",
    "        remote training/validation sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The size of the combined datasets\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _create_dataset(self):\n",
    "        \"\"\"Create a single list unifying examples from all remote datasets\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the dataset\n",
    "        self.dataset = []\n",
    "      \n",
    "        # populate the dataset list\n",
    "        for dataset in self.sets:\n",
    "            for example in dataset:\n",
    "                self.dataset.append(example)\n",
    "                \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"The collat_fn method to be used by the\n",
    "        PyTorch data loader.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unzip the batch\n",
    "        vectors, targets = list(zip(*batch))        \n",
    "            \n",
    "        # concatenate the vectors\n",
    "        vectors = torch.stack(vectors)\n",
    "        \n",
    "        #concatenate the labels\n",
    "        targets = torch.stack(targets)        \n",
    "        \n",
    "        return vectors, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create two such `DatasetMTS` objects, one for training and the other for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a training Dataset object\n",
    "trainset = DatasetMTS(sets = [train_bob,\n",
    "                               train_alice],\n",
    "                       share_workers = [bob, alice],\n",
    "                       crypto_provider = crypto_provider,\n",
    "                       nlp = nlp\n",
    "                      )\n",
    "\n",
    "# Instantiate a validation Dataset object\n",
    "valset = DatasetMTS(sets = [val_bob,\n",
    "                             val_alice],\n",
    "                     share_workers = [bob, alice],\n",
    "                     crypto_provider = crypto_provider,\n",
    "                     nlp = nlp\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `__getitem__` method to obtain the embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training Vector size is 300\n"
     ]
    }
   ],
   "source": [
    "vec_enc, label_enc = trainset.__getitem__(1)\n",
    "print(f' Training Vector size is {vec_enc.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Validation Vector size is 300\n"
     ]
    }
   ],
   "source": [
    "vec_enc, label_enc = valset.__getitem__(1)\n",
    "print(f' Validation Vector size is {vec_enc.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configuration\n",
    "\n",
    "We will now describe the training hyper-parameters for training and validation and create the PyTorch data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model hyper-parameters constants\n",
    "# batch=32 and LR=0.001 are the original setup\n",
    "# batch=32 and LR=0.0025 also work \n",
    "EMBED_DIM = vec_enc.shape[0]\n",
    "BATCH_SIZE = 32 #16, 64 worked\n",
    "LEARNING_RATE = 0.0025 # 0.1, 0.05 worked\n",
    "EPOCHS = 2 # Complete passes in the data\n",
    "NUN_CLASS = 2 # 2 classes since its a binary classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DataLoader object for the training set\n",
    "trainloader = DataLoader(trainset, shuffle = True,\n",
    "                         batch_size = BATCH_SIZE, num_workers = 0, \n",
    "                         collate_fn = trainset.collate_fn)\n",
    "\n",
    "\n",
    "# Instantiate the DataLoader object for the validation set\n",
    "valloader = DataLoader(valset, shuffle = True,\n",
    "                       batch_size = BATCH_SIZE, num_workers = 0, \n",
    "                       collate_fn = valset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an encrypted classifier model\n",
    "The classifier we will use is a simple fully connected network with 300 input features which is the size of the embedding vectors computed previously by SyferText. The network is a binary classifier with an outputs for surgical specialties and another one for every other specialty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.fc = torch.nn.Linear(in_features, out_features)\n",
    "                \n",
    "    def forward(self, x):\n",
    "       \n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        probs = F.relu(logits)\n",
    "        \n",
    "        return probs, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize and encrypt the classifier. The encryption here must use the same workers that hold the share and the same primitives used to encrypt the document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py:79: UserWarning: Use dtype instead of field\n",
      "  warnings.warn(\"Use dtype instead of field\")\n",
      "/home/carlos/anaconda3/envs/pysyft/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/additive_shared.py:91: UserWarning: Default args selected\n",
      "  warnings.warn(\"Default args selected\")\n"
     ]
    }
   ],
   "source": [
    "# Create the classifer\n",
    "model = Classifier(in_features = EMBED_DIM, out_features = NUN_CLASS)\n",
    "\n",
    "# Apply SMPC encryption\n",
    "model = model.fix_precision().share(bob, alice, \n",
    "                                              crypto_provider = crypto_provider,\n",
    "                                              requires_grad = True\n",
    "                                              )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing to do before training is creating an optimizer. The optimizer doesn't need to be encrypted since it operates separately within each worker holding the classifier and the embeddings' shares. One thing to note is that the optimizer needs to operate on fixed precision numbers to be able to encode shares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize stochastic gradient descent (SGD) optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(params = model.parameters(),\n",
    "                  lr = LEARNING_RATE)\n",
    "\n",
    "optimizer = optimizer.fix_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: FixedPrecisionTensor>tensor(2)\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Tensorboard\n",
    "We need to create a summary writer in order to view the training and validation curves for loss and accuracy.\n",
    "Then we will be able to run `Tensorboard` and see the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary writer for logging performance with Tensorboard\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Open a terminal, navigate to the folder containing this notebook, and run:\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir runs/\n",
    "```\n",
    "\n",
    "Then open you favorite web browser and go to `localhost:6006`.\n",
    "\n",
    "You should now be able to see performance curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the below cell to launch the training.\n",
    "`NLLLoss()` is not yet implemented in PySyft for SMPC mode so we will use Mean Squared Error (MSE) as a training loss even though is not the best choice for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \tLoss: 32.04(train)\t|\tAcc: 34.38%(train)\n",
      "epoch: 1 \tLoss: 32.00(valid)\t|\tAcc: 31.25%(valid)\n",
      "epoch: 1 \tLoss: 32.00(train)\t|\tAcc: 25.00%(train)\n",
      "epoch: 1 \tLoss: 32.01(valid)\t|\tAcc: 31.25%(valid)\n",
      "epoch: 1 \tLoss: 32.00(train)\t|\tAcc: 21.88%(train)\n",
      "epoch: 1 \tLoss: 32.00(valid)\t|\tAcc: 18.75%(valid)\n",
      "epoch: 1 \tLoss: 32.03(train)\t|\tAcc: 25.00%(train)\n",
      "epoch: 1 \tLoss: 32.00(valid)\t|\tAcc: 25.00%(valid)\n",
      "epoch: 1 \tLoss: 32.00(train)\t|\tAcc: 25.00%(train)\n",
      "epoch: 1 \tLoss: 31.93(valid)\t|\tAcc: 37.50%(valid)\n",
      "epoch: 1 \tLoss: 31.94(train)\t|\tAcc: 31.25%(train)\n",
      "epoch: 1 \tLoss: 32.00(valid)\t|\tAcc: 3.12%(valid)\n",
      "epoch: 1 \tLoss: 31.70(train)\t|\tAcc: 28.12%(train)\n",
      "epoch: 1 \tLoss: 27.95(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 28.15(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 14.02(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 15.33(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 1 \tLoss: 9.93(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 10.13(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 13.10(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 9.32(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 12.88(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 12.76(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 7.77(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 1 \tLoss: 11.09(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 7.42(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 1 \tLoss: 11.55(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 16.90(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 9.16(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 10.83(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 15.80(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 9.31(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 11.36(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 13.24(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 21.79(train)\t|\tAcc: 59.38%(train)\n",
      "epoch: 1 \tLoss: 14.75(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 13.36(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 10.31(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 14.92(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 13.08(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 10.44(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 7.24(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 1 \tLoss: 10.30(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 10.29(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 14.58(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 7.37(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 1 \tLoss: 12.09(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 13.37(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 18.90(train)\t|\tAcc: 62.50%(train)\n",
      "epoch: 1 \tLoss: 16.33(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 14.77(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 10.16(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 14.53(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 9.24(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 12.91(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 1 \tLoss: 4.24(valid)\t|\tAcc: 93.75%(valid)\n",
      "epoch: 1 \tLoss: 11.54(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 20.53(valid)\t|\tAcc: 59.38%(valid)\n",
      "epoch: 1 \tLoss: 8.21(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 10.19(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 15.57(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 11.73(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 10.20(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 15.68(valid)\t|\tAcc: 62.50%(valid)\n",
      "epoch: 1 \tLoss: 13.39(train)\t|\tAcc: 65.62%(train)\n",
      "epoch: 1 \tLoss: 11.93(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 12.29(train)\t|\tAcc: 65.62%(train)\n",
      "epoch: 1 \tLoss: 11.44(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 8.10(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 8.20(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 8.78(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 7.50(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 13.54(train)\t|\tAcc: 59.38%(train)\n",
      "epoch: 1 \tLoss: 14.70(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 1 \tLoss: 10.80(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 9.40(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 9.42(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 7.91(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 12.03(train)\t|\tAcc: 59.38%(train)\n",
      "epoch: 1 \tLoss: 8.64(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 8.62(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 9.45(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 8.20(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 8.86(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 8.65(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 6.79(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 10.54(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 1 \tLoss: 9.37(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 10.44(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 7.02(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 1 \tLoss: 7.27(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 10.63(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 6.53(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 9.52(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 8.79(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 10.31(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 9.95(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 8.07(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 1 \tLoss: 7.43(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 12.92(valid)\t|\tAcc: 62.50%(valid)\n",
      "epoch: 1 \tLoss: 6.09(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 6.43(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 5.16(train)\t|\tAcc: 90.62%(train)\n",
      "epoch: 1 \tLoss: 8.46(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 7.39(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 8.48(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 13.29(train)\t|\tAcc: 65.62%(train)\n",
      "epoch: 1 \tLoss: 6.54(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 9.96(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 1 \tLoss: 12.03(valid)\t|\tAcc: 62.50%(valid)\n",
      "epoch: 1 \tLoss: 9.46(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 8.72(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 1 \tLoss: 10.74(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 7.91(valid)\t|\tAcc: 90.62%(valid)\n",
      "epoch: 1 \tLoss: 11.01(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 10.92(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 12.58(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 10.71(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 11.05(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 8.85(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 11.24(train)\t|\tAcc: 62.50%(train)\n",
      "epoch: 1 \tLoss: 13.18(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 13.21(train)\t|\tAcc: 65.62%(train)\n",
      "epoch: 1 \tLoss: 11.25(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 12.29(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 10.59(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 10.38(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 9.71(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 9.86(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 1 \tLoss: 8.32(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 13.05(train)\t|\tAcc: 62.50%(train)\n",
      "epoch: 1 \tLoss: 10.51(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 12.00(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 12.00(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 9.67(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 9.89(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 12.41(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 7.53(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 11.89(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 6.97(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 9.80(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 7.68(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 10.14(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 7.59(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 11.02(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 13.75(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 1 \tLoss: 8.28(train)\t|\tAcc: 87.50%(train)\n",
      "epoch: 1 \tLoss: 11.31(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 7.19(train)\t|\tAcc: 87.50%(train)\n",
      "epoch: 1 \tLoss: 11.53(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 1 \tLoss: 7.08(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 10.08(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 10.83(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 8.40(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 7.42(train)\t|\tAcc: 87.50%(train)\n",
      "epoch: 1 \tLoss: 7.91(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 9.08(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 9.91(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 6.20(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 7.97(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 6.74(train)\t|\tAcc: 87.50%(train)\n",
      "epoch: 1 \tLoss: 11.97(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 6.76(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 10.43(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 1 \tLoss: 11.07(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 9.00(valid)\t|\tAcc: 68.75%(valid)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \tLoss: 13.18(train)\t|\tAcc: 65.62%(train)\n",
      "epoch: 1 \tLoss: 6.53(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 10.97(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 9.98(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 9.46(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 8.10(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 11.69(train)\t|\tAcc: 65.62%(train)\n",
      "epoch: 1 \tLoss: 12.33(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 7.66(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 8.97(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 8.95(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 1 \tLoss: 8.50(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 7.67(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 12.50(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 1 \tLoss: 10.06(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 1 \tLoss: 9.00(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 9.61(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 10.20(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 10.23(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 5.42(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 6.69(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 9.96(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 12.76(train)\t|\tAcc: 62.50%(train)\n",
      "epoch: 1 \tLoss: 9.71(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 10.39(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 1 \tLoss: 8.16(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 1 \tLoss: 7.96(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 13.99(valid)\t|\tAcc: 59.38%(valid)\n",
      "epoch: 1 \tLoss: 7.05(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 1 \tLoss: 8.47(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 5.28(train)\t|\tAcc: 90.62%(train)\n",
      "epoch: 1 \tLoss: 12.82(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 1 \tLoss: 14.85(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 8.85(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 10.37(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 8.12(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 1 \tLoss: 13.14(train)\t|\tAcc: 62.50%(train)\n",
      "epoch: 1 \tLoss: 9.92(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 1 \tLoss: 10.95(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 9.17(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 11.12(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 1 \tLoss: 11.16(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 1 \tLoss: 8.73(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 1 \tLoss: 6.39(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 11.44(train)\t|\tAcc: 62.50%(train)\n",
      "epoch: 1 \tLoss: 11.38(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 1 \tLoss: 8.83(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 1 \tLoss: 6.30(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 1 \tLoss: 7.80(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 1 \tLoss: 8.30(valid)\t|\tAcc: 90.62%(valid)\n",
      "epoch: 1 \tLoss: 3.85(train)\t|\tAcc: 56.25%(train)\n",
      "epoch: 1 \tLoss: 13.72(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 2 \tLoss: 13.07(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 2 \tLoss: 8.47(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 2 \tLoss: 8.70(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 8.50(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 8.14(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 2 \tLoss: 15.62(valid)\t|\tAcc: 56.25%(valid)\n",
      "epoch: 2 \tLoss: 11.06(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 2 \tLoss: 7.85(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 2 \tLoss: 8.95(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 9.95(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 9.36(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 2 \tLoss: 12.62(valid)\t|\tAcc: 62.50%(valid)\n",
      "epoch: 2 \tLoss: 10.71(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 7.57(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 2 \tLoss: 7.66(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 10.45(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 12.10(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 2 \tLoss: 9.37(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 8.26(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 2 \tLoss: 7.70(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 2 \tLoss: 9.50(train)\t|\tAcc: 65.62%(train)\n",
      "epoch: 2 \tLoss: 11.19(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 2 \tLoss: 10.70(train)\t|\tAcc: 62.50%(train)\n",
      "epoch: 2 \tLoss: 10.63(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 2 \tLoss: 10.24(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 2 \tLoss: 6.15(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 2 \tLoss: 7.70(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 2 \tLoss: 13.33(valid)\t|\tAcc: 59.38%(valid)\n",
      "epoch: 2 \tLoss: 6.64(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 2 \tLoss: 9.12(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 5.96(train)\t|\tAcc: 87.50%(train)\n",
      "epoch: 2 \tLoss: 5.03(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 2 \tLoss: 8.40(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 2 \tLoss: 6.92(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 2 \tLoss: 11.40(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 2 \tLoss: 8.06(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 7.34(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 2 \tLoss: 10.98(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 2 \tLoss: 8.50(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 2 \tLoss: 9.50(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 2 \tLoss: 7.66(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 2 \tLoss: 4.09(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 2 \tLoss: 12.16(train)\t|\tAcc: 62.50%(train)\n",
      "epoch: 2 \tLoss: 9.41(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 9.13(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 2 \tLoss: 9.81(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 5.29(train)\t|\tAcc: 87.50%(train)\n",
      "epoch: 2 \tLoss: 9.48(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 5.46(train)\t|\tAcc: 90.62%(train)\n",
      "epoch: 2 \tLoss: 4.87(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 2 \tLoss: 7.69(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 8.69(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 8.32(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 8.54(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 2 \tLoss: 3.15(train)\t|\tAcc: 93.75%(train)\n",
      "epoch: 2 \tLoss: 9.22(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 13.90(train)\t|\tAcc: 62.50%(train)\n",
      "epoch: 2 \tLoss: 7.90(valid)\t|\tAcc: 90.62%(valid)\n",
      "epoch: 2 \tLoss: 9.22(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 2 \tLoss: 7.96(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 8.29(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 9.82(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 2 \tLoss: 9.59(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 2 \tLoss: 12.65(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 2 \tLoss: 8.03(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 8.75(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 10.72(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 2 \tLoss: 8.90(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 10.43(train)\t|\tAcc: 65.62%(train)\n",
      "epoch: 2 \tLoss: 10.70(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 2 \tLoss: 7.72(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 2 \tLoss: 7.23(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 7.16(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 2 \tLoss: 9.89(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 2 \tLoss: 10.43(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 2 \tLoss: 6.38(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 2 \tLoss: 8.16(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 2 \tLoss: 13.48(valid)\t|\tAcc: 59.38%(valid)\n",
      "epoch: 2 \tLoss: 11.27(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 2 \tLoss: 11.43(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 2 \tLoss: 10.15(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 2 \tLoss: 9.52(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 11.83(train)\t|\tAcc: 65.62%(train)\n",
      "epoch: 2 \tLoss: 6.45(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 2 \tLoss: 7.05(train)\t|\tAcc: 90.62%(train)\n",
      "epoch: 2 \tLoss: 10.95(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 2 \tLoss: 7.79(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 10.12(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 2 \tLoss: 9.49(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 2 \tLoss: 8.53(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 2 \tLoss: 9.30(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 2 \tLoss: 6.72(valid)\t|\tAcc: 93.75%(valid)\n",
      "epoch: 2 \tLoss: 8.27(train)\t|\tAcc: 87.50%(train)\n",
      "epoch: 2 \tLoss: 10.16(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 7.62(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 2 \tLoss: 8.19(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 2 \tLoss: 7.74(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 9.22(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 8.75(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 9.60(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 9.27(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 9.91(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 2 \tLoss: 6.45(train)\t|\tAcc: 90.62%(train)\n",
      "epoch: 2 \tLoss: 10.14(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 2 \tLoss: 8.74(train)\t|\tAcc: 68.75%(train)\n",
      "epoch: 2 \tLoss: 6.72(valid)\t|\tAcc: 87.50%(valid)\n",
      "epoch: 2 \tLoss: 7.56(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 7.82(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 2 \tLoss: 8.90(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 8.33(valid)\t|\tAcc: 71.88%(valid)\n",
      "epoch: 2 \tLoss: 7.30(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 2 \tLoss: 7.91(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 4.28(train)\t|\tAcc: 96.88%(train)\n",
      "epoch: 2 \tLoss: 5.71(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 2 \tLoss: 8.53(train)\t|\tAcc: 75.00%(train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 \tLoss: 9.03(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 11.54(train)\t|\tAcc: 59.38%(train)\n",
      "epoch: 2 \tLoss: 9.41(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 8.29(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 9.76(valid)\t|\tAcc: 65.62%(valid)\n",
      "epoch: 2 \tLoss: 7.66(train)\t|\tAcc: 87.50%(train)\n",
      "epoch: 2 \tLoss: 6.02(valid)\t|\tAcc: 81.25%(valid)\n",
      "epoch: 2 \tLoss: 9.43(train)\t|\tAcc: 75.00%(train)\n",
      "epoch: 2 \tLoss: 10.60(valid)\t|\tAcc: 78.12%(valid)\n",
      "epoch: 2 \tLoss: 8.52(train)\t|\tAcc: 81.25%(train)\n",
      "epoch: 2 \tLoss: 7.58(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 2 \tLoss: 6.97(train)\t|\tAcc: 84.38%(train)\n",
      "epoch: 2 \tLoss: 8.30(valid)\t|\tAcc: 75.00%(valid)\n",
      "epoch: 2 \tLoss: 8.72(train)\t|\tAcc: 78.12%(train)\n",
      "epoch: 2 \tLoss: 4.47(valid)\t|\tAcc: 90.62%(valid)\n",
      "epoch: 2 \tLoss: 9.15(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 2 \tLoss: 10.18(valid)\t|\tAcc: 68.75%(valid)\n",
      "epoch: 2 \tLoss: 7.95(train)\t|\tAcc: 71.88%(train)\n",
      "epoch: 2 \tLoss: 5.56(valid)\t|\tAcc: 84.38%(valid)\n",
      "epoch: 2 \tLoss: 6.98(train)\t|\tAcc: 78.12%(train)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "        \n",
    "    for iter, (vectors, targets) in enumerate(trainloader):\n",
    "        \n",
    "        # Set train mode        \n",
    "        model.train() \n",
    "\n",
    "        # 1). Zero out previous gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2). predict sentiment probabilities        \n",
    "        probs, logits = model(vectors)\n",
    "\n",
    "        # 3). Compute loss and accuracy\n",
    "        \n",
    "        loss = ((probs -  targets)**2).sum().refresh()#/BATCH_SIZE\n",
    "        \n",
    "        # Get the predicted labels\n",
    "        preds = probs.argmax(dim=1)\n",
    "        targets = targets.argmax(dim=1)\n",
    "        \n",
    "        # Compute the prediction accuracy\n",
    "        accuracy = preds.eq(targets).sum()\n",
    "        accuracy = accuracy.get().float_precision()\n",
    "        accuracy = 100 * (accuracy / BATCH_SIZE)\n",
    "        \n",
    "        # 4). Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5). Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Decrypt the loss for logging\n",
    "        loss = loss.get().float_precision()\n",
    "        \n",
    "        # print progress in training    \n",
    "        print(\"epoch:\",epoch+1,f'\\tLoss: {loss.item():.2f}(train)\\t|\\tAcc: {accuracy:.2f}%(train)')        \n",
    "        \n",
    "        # Log to tensorboard\n",
    "        writer.add_scalar('train/loss', loss.item(), epoch * len(trainloader) + iter )\n",
    "        writer.add_scalar('train/acc', accuracy, epoch * len(trainloader) + iter )\n",
    "\n",
    "            \n",
    "        \"\"\" Perform validation on exactly one batch \"\"\"\n",
    "        \n",
    "        # Set validation mode        \n",
    "        model.eval()     \n",
    "    \n",
    "        for vectors, targets in valloader:            \n",
    "            \n",
    "            probs, logits = model(vectors)\n",
    "\n",
    "            loss = ((probs -  targets)**2).sum().refresh()#/BATCH_SIZE\n",
    "\n",
    "            preds = probs.argmax(dim=1)\n",
    "            targets = targets.argmax(dim=1)\n",
    "\n",
    "            accuracy = preds.eq(targets).sum()\n",
    "            accuracy = accuracy.get().float_precision()\n",
    "            accuracy = 100 * (accuracy / BATCH_SIZE)\n",
    "\n",
    "            loss = loss.get().float_precision()\n",
    "            \n",
    "            # print progress in validation            \n",
    "            print(\"epoch:\",epoch+1,f'\\tLoss: {loss.item():.2f}(valid)\\t|\\tAcc: {accuracy:.2f}%(valid)')\n",
    "\n",
    "            # Log to tensorboard\n",
    "            writer.add_scalar('val/loss', loss.item(), epoch * len(trainloader) + iter )\n",
    "            writer.add_scalar('val/acc', accuracy, epoch * len(trainloader) + iter )\n",
    "            \n",
    "            break\n",
    "            \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Now that training is finished, you can verify that both Bob and Alice have `SubPipeline` objects on their machines containing the pipeline components defined previously in **Figure 2**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to save the model, train the model again but do not use the Tensorboard `SummaryWriter`, otherwise the next cell will produce an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On bob's machine\n",
    "[bob._objects[id] for id in bob._objects if  isinstance(bob._objects[id], syfertext.SubPipeline)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Alices's machine\n",
    "[alice._objects[id] for id in alice._objects if  isinstance(alice._objects[id], syfertext.SubPipeline)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_name = \"mt_classifier_\" \n",
    "model_specs = str(EPOCHS) + \"epochs_\" + str(LEARNING_RATE) + \"lr_\" + str(BATCH_SIZE) + \"bsize\"\n",
    "PATH = \"../models/\" + model_name + model_specs + \".pth\"\n",
    "\n",
    "torch.save(model.state_dict(),PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
